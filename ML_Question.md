#  ML Question Summary

## Train Vali Test data split 하는 이유

모델이 학습데이터 이외의 데이터에도 올바른 결과를 도출 해 내야하기 때문에 
train(학습) : 모델을 학습시키는데 필요함
vali(검증) : 학습한 모델이 적합한지 평가 (학습도중 이루어짐)
test(평가): 최종적으로 선택한 모델의 성능평가

## 경사하강법?

경사하강법은 loss function의 임의의 w위치. 에서의 미분값을 참조해 그 지점의 경사를 구하고 그 경사가 0에가까워 지는 w의 위치를 판별해 그 쪽으로 점진적으로 움직여 정확도가 높은 w값을 찾는 방법이다.

## 학습률?

학습률은 경사하강법에서의 움직이는 '양' 
높으면 빠르지만 도달하지 못할 위험이 있고 낮으면 속도가 느린 단점이 있다

## 확률적 경사 하강법(SGD)

경사하강버븐 모든 데이터에대해 기울기값을 모두 구해야 정확하게 나아갈수있는데
데이터가 많을경우 계산이 많아져서 힘들다. 그 데이터의 양을 줄여서 몇개의 데이터만 참조해서 나아가는 방법이 확률적 경사 하강법.

## 선형회귀?

y=wx+b의 형태의 수식을띄는것. y는 x값(데이터)가 주어질경우 도달하는 정답.

## 회귀 vs 분류

회귀는 실수값. 즉 퍼지논리를 따르는 연속성 값들중에 하나의 값을 찾는것(mse사용)
분류는 카테고리 값을 예측하는 모델 (cross entropy사용)

## 과적합 and 일반화

과적합이란 모델이 학습데이터를 지나치게 학습했을때 나타나는 현상. 학습데이터 이외의 데이터 즉 우리가 적용해야할 모르는 데이터에 대응할수 없는 현상.
일반화란 과적합을 막아주는 방법 L2,L1,Dropout,BN등의 방법을 사용하면 일반화를 할 수 있다.

## 데이터 표현이란?

원시데이터(raw data)를 특성 벡터(feture vector)에 매핑한 결과. 즉 원시데이터를 전처리하는것이 표현 데이터 표현은 특성 추출로 이루어진다.

## 좋은 특성이란?

데이터의 전처리를 하는 이유는 좋은 특성을 얻기 위함이다. 그러기 위해선
1.거의 사용되지 않는 불연속 특성값 배제
2.가급적 분명하고 명확한 의미 부여
3.특수값을 실제 데이터와 혼용하지 말아야 한다.
4.시간이 지나도 변하지 않아야한다.

## 선형 vs 비선형

선형은 선 '하나' 로 데이터를 분류하고 비선형은 선하나로 데이터를 분류할 수 없는 문제
선형 : y = wx +b
비선형 : y = w1x1 + w2x2 ... + b

## L2 정규화란?

모델은 Loss의 값을 줄이려고 합니다. 고로 loss function안에 설정된 L2의 값도 줄이려고 합니다. L2란 모델의 모든 가중치 제곱의 합. 그렇다면 비정상적으로 값이 큰 가중치는 학습을 하면 할수록 줄어들게 됩니다. 큰 가중치 값이 줄어듬으로써 모델은 복잡성이 낮아지고 오버피팅을 방지 할 수 있습니다.

## L2의 람다

특정한 스칼라값. L2항에 곱해줌으로써 L2항을 얼마나 믿어야할지 정해주는 하이퍼 파라미터입니다. L2정규화는 기능을 방해하는 가중치값을 0으로 유도하고 가중치의 평균을 0으로 유도하는 일을 합니다. 고로 람다값을 높이면 가중치가 좀더 0에 가까워지게 되며, 람다값을 낮추면 가중치가 좀더 커지게 됩니다. 즉 람다의 목적은, 과적합과 과소적합 사이에 존재하는 최적값을 도출 하는것.

## 임계값

로지스틱 회귀 값을 이진 카테고리에 매핑할때 쓰이는 기준점. 

## TP, TN, FP, FN

## precision/recall

precision :  TP / TP + FP

recall : 모델이 몇%확률로 정확히 예측하는 지표 TP / TP + FN


## L1 정규화?

L1정규화도 loss function에서 가중치를 줄여주는 역할을 합니다.
너무 많은 차원이 존재하면 모델이 커져서 많은 메모리를 필요로함. 하지만 모든 가중치가 모두 유용하지는 않음. 불필요한 가중치를 0으로 만들어버리는 정규화. L2정규화는 가중치를 x%만큼 줄여나가기때문에 절대 0이 되지않습니다. 하지만 L1은 가중치의 절대값에 패널티를 주기 때문에 불필요한 가중치는 0으로 변하여 제거됩니다.

## 활성화 함수?

비선형 문제를 모델링 하기위해 활성화 함수를 도입. 각 히든 레이어의 노드가 비선형 함수를 통과하도록 할 수 있다. sigmoid > ReLU

## 경사소실

딥러닝 층이 깊어질 수록 Gradient가 전달되지 않는 현상. 활성화 함수의 특성때문에 모델이 깊어 질 수록 기울기는 점점 0에 수렴하게 됩니다. 예로 시그모이드를 미분하면 최대치가 0.3입니다. 1보다 작으므로 계속 곱하다보면 0에 가까워 집니다.

## 경사발산

Gradient가 누적되어 매우 큰 값이 될때 모델이 불안정해지고 학습이 되지않는 상태
해결방법 : gradient_clipping

## ReLu 유닛 소멸

학습을하며 가중치가 업데이트 되는데 업데이트 되는 가중치들의 합이 음수가 될경우 Relu함수는 0이라는 값을 뱉어내게 됩니다.(이후의 레이어에서 학습이 진행되지 않음) 이것이 Relu 유닛 소멸, 학습률을 낮추면 유닛소멸을 방지 할 수 있습니다. ReLU대신 Leaky ReLU나 Maxout같은 방법도 사용 할 수 있습니다.

## 드롭아웃

노드중 일부를 제거하면서 학습하는 방법. 간단히 말하면 학습을 덜하는 방법 입니다. 얼마나 덜 할지 선택 할 수 있으며, 오버피팅의 문제를 해결, 영향이 강한 뉴런의 가중치의 영향을 줄이며 정규화의 효과가 있습니다.

## 소프트맥스

소프트맥스는 다중 클래스의 확률을 생성해내는 '활성화 함수'입니다. ‘사과, 배, 오렌지, 포도’ 라는 클래스들이 있을경우 각각의 확률을 생성해내며 생성한 값(이경우 4개)의 합은 1이 됩니다. 주로 분류모델의 마지막 레이어에 softmax를 넣어 출력값을 확률분포로 변경합니다.
